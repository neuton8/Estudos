{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt-3.5-turbo\"\n",
    "chat = ChatOpenAI(model_name=model_name, temperature=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Como fazemos os prompts?\n",
    "OS prompts são feitos com os espaços das variáveis, servindo como pergunta para a LLM. Uma coisa legal é que tudo é independente, então voce pode usar diferentes modelos para diferentes situações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[HumanMessage(content='Quanto é 3 + 2?')]\n"
     ]
    }
   ],
   "source": [
    "template = ChatPromptTemplate.from_template(\"Quanto é {valor} + 2?\")\n",
    "resposta = template.invoke({'valor':3})\n",
    "print(resposta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='3 + 2 é igual a 5.', response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 16, 'total_tokens': 26}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-56170964-1ba7-41cf-af0b-3c89bb5c3e53-0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.invoke(\"Quanto é 3 + 2?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequencia de Prompts\n",
    "Nesse caso usamos o conceito de _runnable interface_, que é basicamente uma sequência de atos para obter um resultado. Um exemplo abaixo será feito no caso de uma viagem. Importante lembrar que o output geralmente não é uma string e nesse caso é muito importante fazer um parse da saída"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_viagem = ChatPromptTemplate.from_template(\"Quero viajar para {destino} e gostaria de saber mais informações sobre o local.\")\n",
    "template_dias = ChatPromptTemplate.from_template(\"Vou ficar {dias} dias e preciso de um roteiro de viagem.\")\n",
    "template_orcamento = ChatPromptTemplate.from_template(\"Meu orçamento é de R$ {valor} e gostaria de saber se é possível para esse roteiro.\")\n",
    "\n",
    "\n",
    "from langchain.callbacks.tracers import ConsoleCallbackHandler\n",
    "\n",
    "\n",
    "# nesse caso, a saída do primeiro entra no segundo, e a saída do segundo entra no terceiro\n",
    "chain = template_viagem | chat | StrOutputParser() | template_dias | chat | StrOutputParser() | template_orcamento | chat | StrOutputParser()\n",
    "resposta = chain.invoke({'destino':'Paris', 'dias':5, 'valor':5000},config={'callbacks': [ConsoleCallbackHandler()]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tem como voce indicar no prompt para ele te dar apenas uma palavra e tudo mais. O langchain nos entrega uma forma de analisar o output e pegar a parte mais importante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import Field, BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"destino\": \"Paris\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"destino\": \"Paris\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Quero viajar para Paris e gostaria de saber mais informações sobre o local./n The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\\\"properties\\\": {\\\"foo\\\": {\\\"title\\\": \\\"Foo\\\", \\\"description\\\": \\\"a list of strings\\\", \\\"type\\\": \\\"array\\\", \\\"items\\\": {\\\"type\\\": \\\"string\\\"}}}, \\\"required\\\": [\\\"foo\\\"]}\\nthe object {\\\"foo\\\": [\\\"bar\\\", \\\"baz\\\"]} is a well-formatted instance of the schema. The object {\\\"properties\\\": {\\\"foo\\\": [\\\"bar\\\", \\\"baz\\\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\\\"properties\\\": {\\\"cidade\\\": {\\\"title\\\": \\\"Cidade\\\", \\\"default\\\": \\\"Apenas o nome da cidade\\\", \\\"type\\\": \\\"string\\\"}, \\\"lista_atracoes\\\": {\\\"title\\\": \\\"Lista Atracoes\\\", \\\"default\\\": \\\"Lista com 3 atra\\\\u00e7\\\\u00f5es tur\\\\u00edsticas\\\", \\\"type\\\": \\\"array\\\", \\\"items\\\": {}}}}\\n```\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOpenAI] [1.07s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\n\\t\\\"cidade\\\": \\\"Paris\\\",\\n\\t\\\"lista_atracoes\\\": [\\\"Torre Eiffel\\\", \\\"Museu do Louvre\\\", \\\"Catedral de Notre-Dame\\\"]\\n}\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\n\\t\\\"cidade\\\": \\\"Paris\\\",\\n\\t\\\"lista_atracoes\\\": [\\\"Torre Eiffel\\\", \\\"Museu do Louvre\\\", \\\"Catedral de Notre-Dame\\\"]\\n}\",\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 40,\n",
      "                \"prompt_tokens\": 227,\n",
      "                \"total_tokens\": 267\n",
      "              },\n",
      "              \"model_name\": \"gpt-3.5-turbo\",\n",
      "              \"system_fingerprint\": null,\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-3b8775eb-4989-4fda-b2ac-0262783f1f3f-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 40,\n",
      "      \"prompt_tokens\": 227,\n",
      "      \"total_tokens\": 267\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:JsonOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"cidade\": \"Paris\",\n",
      "  \"lista_atracoes\": [\n",
      "    \"Torre Eiffel\",\n",
      "    \"Museu do Louvre\",\n",
      "    \"Catedral de Notre-Dame\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [1.08s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"cidade\": \"Paris\",\n",
      "  \"lista_atracoes\": [\n",
      "    \"Torre Eiffel\",\n",
      "    \"Museu do Louvre\",\n",
      "    \"Catedral de Notre-Dame\"\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cidade': 'Paris',\n",
       " 'lista_atracoes': ['Torre Eiffel',\n",
       "  'Museu do Louvre',\n",
       "  'Catedral de Notre-Dame']}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.globals import set_debug\n",
    "from langchain_core.output_parsers import StrOutputParser,JsonOutputParser\n",
    "\n",
    "set_debug(True)\n",
    "\n",
    "# Utilizando o pydantic, craimos dataclasses e especificamos como queremos nosso output. Esse tipo de script cria no prompt uma forma de receber a resposta a partir de um formato específico, justamente o que a função get_format_instructions() faz.\n",
    "class CamposSaida(BaseModel):\n",
    "    cidade: str = Field(\"Apenas o nome da cidade\")\n",
    "    lista_atracoes: list = Field(\"Lista com 3 atrações turísticas\")\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=CamposSaida)\n",
    "\n",
    "template_viagem = PromptTemplate(template = \"Quero viajar para {destino} e gostaria de saber mais informações sobre o local./n {resposta}\",input_variables=['destino'],partial_variables={'resposta':parser.get_format_instructions()})\n",
    "chain_cidade = template_viagem | chat \n",
    "chain_cidade.invoke({'destino':'Paris'})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
